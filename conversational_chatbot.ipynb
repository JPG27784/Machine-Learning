{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A conversation Chatbot for an E-Commerce Application"
      ],
      "metadata": {
        "id": "CkaZ15bkt6SM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS3n2y8wPw5g"
      },
      "source": [
        "## Basics of RAG\n",
        "\n",
        "Before we start coding, lets go over a few questions and get it clarified.\n",
        "\n",
        "### 1. What is RAG?\n",
        "Imagine you're writing an article about climate change, but instead of relying only on what you remember, you search online for recent studies and data to support your writing. RAG works the same way—it combines a powerful AI language model with a search system that retrieves relevant information from an external knowledge source, like a database or documents. This helps generate more accurate and informative responses.\n",
        "\n",
        "### 2. Why is RAG important?\n",
        "RAG is crucial because AI models, like ChatGPT, can sometimes \"hallucinate\" or provide outdated or incorrect information. By retrieving facts from trusted sources before generating responses, RAG ensures the answers are more reliable, up-to-date, and contextually relevant.\n",
        "\n",
        "### 3. What’s the difference between RAG and a standard AI chatbot?\n",
        "A standard chatbot relies only on pre-trained knowledge, which may be limited or outdated. RAG-enhanced chatbots, however, actively retrieve fresh, relevant information from external sources, ensuring better accuracy and up-to-date insights.\n",
        "\n",
        "### 4. What are the key components of RAG?\n",
        "RAG consists of two main parts:\n",
        "\n",
        "- Retriever: Finds the most relevant documents or data from a knowledge base (e.g., a search engine or database).\n",
        "- Generator: Uses the retrieved information to produce a coherent and accurate response.\n",
        "\n",
        "### 5. Usecases in real-life\n",
        "- Customer Support: AI chatbots retrieve knowledge base articles to provide better responses to customer queries.\n",
        "- Healthcare: Doctors can get AI-assisted summaries of patient records and the latest medical research.\n",
        "- Legal Services: Lawyers can search through legal documents and case studies to build stronger cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9ZmExzDRva"
      },
      "source": [
        "### Understanding the Limitation of the LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install"
      ],
      "metadata": {
        "id": "IAYqd-7Dzmh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf100878-7688-4e70-ba3f-66a2015cd55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRit5A9KCx20"
      },
      "outputs": [],
      "source": [
        "# Importing the OpenAI library to interact with OpenAI's API services.\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nKBjz45zDj6r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "73696b0b-cf26-4c87-f115-5cb6431db6a5"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2451157930.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Prompting the user to securely enter their OpenAI API key without displaying it on the screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mOPENAI_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your OpenAI API key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             )\n\u001b[0;32m-> 1159\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os  # Importing the os module to interact with environment variables\n",
        "import getpass  # Importing getpass to securely input sensitive information\n",
        "\n",
        "# Prompting the user to securely enter their OpenAI API key without displaying it on the screen\n",
        "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7YY7aEjDj9I"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of the OpenAI client using the provided API key.\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtvWPnanDkAj"
      },
      "outputs": [],
      "source": [
        "# Defining the prompt to query the LLM\n",
        "prompt = ''' What was uber's revenue in 2022? '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBFCA_A_DWBp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "fb4ee9cc-ee17-4825-c9bd-c7129d24738b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: jagadeesh. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3583460071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sending a request to the OpenAI API to generate a chat response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m openai_response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Specifying the model to use;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Note: An older model chosen for testing purposes because the cutoff is 2021 whereas prompt is querying details about 2022\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Creating a structured message for the AI model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m         )\n\u001b[0;32m-> 1294\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: jagadeesh. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ],
      "source": [
        "# Sending a request to the OpenAI API to generate a chat response\n",
        "openai_response = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',  # Specifying the model to use;\n",
        "    # Note: An older model chosen for testing purposes because the cutoff is 2021 whereas prompt is querying details about 2022\n",
        "    messages=[{'role': 'user', 'content': prompt}]  # Creating a structured message for the AI model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WONjF1M1Xs-k"
      },
      "source": [
        "In the above code, while creating a structured message for the AI model,  `role `defines the speaker (user input) and `content` contains the actual query stored in the `prompt` variable.\n",
        "\n",
        "We are structuring the input this way because OpenAI's chat models require a specific format to understand and process conversations effectively. Assigning roles like 'user' helps the AI distinguish between different participants in the conversation, ensuring it provides relevant and context-aware responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2u-Y1hWlDij2",
        "outputId": "7f239c9b-df1f-4998-a17c-4321bb2230bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Uber's revenue in 2022 is not publicly available as the year has not yet come to a close. Companies typically release their annual financial reports after the end of their fiscal year.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Accessing the generated response from the AI model.\n",
        "openai_response.choices[0].message.content\n",
        "# Note:'choices' contains multiple response options, we take the first one ([0]),\n",
        "# 'message' holds the response details, and 'content' extracts the actual text generated by the AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLYV5B_YWGD"
      },
      "source": [
        "### Interpretation:\n",
        "Why is the LLM not able to answer the query?\n",
        "\n",
        "`gpt-3.5-turbo` does not have access to data after 2021 because of its cutoff.\n",
        "\n",
        "#### Do it yourself:\n",
        "Try changing the model to `gpt-4o-mini` and observe how the output changes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl4cl9ylD7AF"
      },
      "source": [
        "As you can see above, the LLM we used(gpt 3.5) doesn't have access to the latest data. Now as LLMs get updated, the training cut-off date may or may not have access to more information. However, it's always a good idea to understand how to improve the context of our prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXHhy-hcEMyy"
      },
      "source": [
        "### Making the LLM context-aware\n",
        "\n",
        "Next Step: Let's check Uber's [financial report ](https://s23.q4cdn.com/407969754/files/doc_events/2024/May/06/2023-annual-report.pdf)\n",
        "\n",
        "On Page 54, of the above document it states:\n",
        "\n",
        "\"Revenue was 37.3 billion, up 17% year-over-year. Mobility revenue increased 5.8 billion primarily attributable to an increase in\n",
        "Mobility Gross Bookings of......\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSAk_2EGD1CC"
      },
      "outputs": [],
      "source": [
        "## Let's create the above context for the prompt\n",
        "# Defining a context string with revenue details retrieved from an external source.\n",
        "retrieved_context = '''Revenue was $37.3 billion, up 17% year-over-year. Mobility revenue increased $5.8 billion primarily attributable to an increase in\n",
        "               Mobility Gross Bookings of 31% year-over-year.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irw2unBKFb0M"
      },
      "outputs": [],
      "source": [
        "## Let's modify our prompt now\n",
        "# Creating a prompt by embedding the retrieved context into a question for the AI model.\n",
        "\n",
        "prompt = f\"What was Uber's revenue in 2022? Check in {retrieved_context}\"\n",
        "\n",
        "# Note: The AI is being asked to analyze the given context and provide Uber's revenue for 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R70ZN0eKFoo7"
      },
      "outputs": [],
      "source": [
        "## Let's ask the LLM again\n",
        "openai_response = client.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': prompt}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "09ddo9F1Fs-w",
        "outputId": "6f570c80-516e-4576-fb5a-8650f0a7d5c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Uber's revenue in 2022 was $37.3 billion.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Accessing the generated response from the AI model.\n",
        "openai_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LuE9mCnbR7u"
      },
      "source": [
        "### Interpretation:\n",
        "How is the LLM able to answer the same question now?\n",
        "\n",
        "The LLM can now answer the question accurately because the relevant financial data is explicitly provided in the `retrieved_context`, allowing the model to reference it directly instead of relying on its pre-trained knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfpLPTtDG57l"
      },
      "source": [
        "As you saw in the example above, we\n",
        "\n",
        "- **retrieved** the context from an external source\n",
        "- **augmented** our prompt that passes to the LLM, and\n",
        "- **generated** the response\n",
        "\n",
        "This is Retrieval Augmented Generation in a nutshell!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKzjA8hmHtaA"
      },
      "source": [
        "### Basic RAG app architecture\n",
        "\n",
        "In the previous example, we manually retrieved the context from the given file which for all purposes is impractical (duh!!)\n",
        "\n",
        "Therefore, we have to devise a strategy that enables us to:\n",
        "\n",
        "- Take the query from the user\n",
        "- Identify the documents from the external source that might be relevant for the query.\n",
        "- Pass those documents' information as context to the LLM\n",
        "- LLM then generates the final response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msqpnuzuJHYY"
      },
      "source": [
        "To do the above, we can follow a simple standard architecture as shown below (Image source - https://huyenchip.com/2024/07/25/genai-platform.html)\n",
        "\n",
        "<center><img src=\"https://huyenchip.com/assets/pics/genai-platform/3-rag.png\" width=500 height=400/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXm08cxzLsia"
      },
      "source": [
        "As you can see in the above image, the retriever would be the key component of this entire architecture.\n",
        "\n",
        "To build the retriever, we have to follow these steps:\n",
        "\n",
        "- Connect to the document source\n",
        "- Break the documents down to manageable chunks. This is due to the fact that taking in the entire document source for building the context will exceed the token limits of the LLM. This process is also called **Chunking**.\n",
        "- Perform a search for the most relevant chunks based on the given query.\n",
        "- Pass those relevant chunks to the LLM.\n",
        "\n",
        "For performing the search or retrieval process, we will be following an **embedding-based approach.**\n",
        "\n",
        "<center><img src=\"https://cdn.prod.website-files.com/640248e1fd70b63c09bd3d09/653fd23f1565c0c1da063efc_Semantic%20Search%20Text%20Embeddings%20(1).png\" width =500/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MPW_40jNLDn"
      },
      "source": [
        "### Understanding Embedding based approach\n",
        "\n",
        "In the embedding based approach:\n",
        "\n",
        "- We convert the document chunks in the database to vector embeddings and store it in a vector store.\n",
        "\n",
        "- Convert the given user query to an embedding.\n",
        "\n",
        "- Find the document chunks whose vector embeddings are closest to the given query embedding using a vector search algorithm like FAISS (Facebook AI Similarity Search)\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*h_btyitJX79d-gFE8RaMQg.png\" width=500/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7aMQOXtON5U"
      },
      "source": [
        "### Tools for building the RAG App\n",
        "\n",
        "Now that we are familiar with the overall architecture, we can now go ahead and structure the tools that we'll use for the upcoming demonstration:\n",
        "\n",
        "- OpenAI LLM (model - GPT 4o-mini): This will be our primary model for generating the responses\n",
        "- LangChain: Langchain is a powerful framework for orchestrating different layers in the RAG app. We shall use this to build the retriever end-to-end and also connect with other tools for tasks such as\n",
        "    - Chunking - RecursiveCharacterTextSplitter\n",
        "    - Embedding Model - OpenAIEmbeddings\n",
        "    - Vector Search Model - FAISS\n",
        "- Gradio: This will help in building a simple UI at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey558a81QcXs"
      },
      "source": [
        "## Building ReviewGenie POC\n",
        "\n",
        "*A basic chatbot that can answer customer queries*\n",
        "\n",
        "## Problem Statement\n",
        "Shopping online can be overwhelming. You search for a simple pair of shoes, but end up scrolling through countless options—many irrelevant, some too expensive, others just not right. Traditional search engines rely on keywords, often missing what you truly need.\n",
        "\n",
        "Let's build an AI-powered product discovery chatbot changes this. Using advanced language models and vector-based search, it goes beyond keywords to understand your intent, offering personalized, context-aware recommendations in seconds.\n",
        "\n",
        "<center><img src=\"https://www.pranathiss.com/static/assets/images/ai-powered-chatBot.webp\" width=500/></center>\n",
        "\n",
        "This smart solution enhances the shopping experience, increasing customer satisfaction, engagement, and conversions. The future of e-commerce is here—smarter, intuitive, and built for you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQfzsw36du3S"
      },
      "source": [
        "### Dataset Used:\n",
        "The given dataset contains information about various products, including their IDs, descriptions, and specifications. Below is a detailed description of each column and the type of data it contains.\n",
        "\n",
        "You can download the entire dataset [here](https://www.kaggle.com/datasets/piyushjain16/amazon-product-data).\n",
        "Or you can download the smaller sample dataset [here](https://drive.google.com/file/d/1ohd9xo19HmDVIwpXPf_IyMkwr29gJJxR/view?usp=drive_link).\n",
        "\n",
        "#### Column Descriptions:\n",
        "- `PRODUCT_ID (Integer)`\n",
        "A unique identifier assigned to each product.\n",
        "Example: 1925202, 2673191\n",
        "- `TITLE (String)`\n",
        "The name or title of the product, usually a brief summary.\n",
        "Example: \"ArtzFolio Tulip Flowers Blackout Curtain for D...\",\n",
        "\"Marks & Spencer Girls' Pyjama Sets T86_2561C_N...\"\n",
        "- `BULLET_POINTS (List of Strings / NaN)`\n",
        "A list of key product features and benefits in bullet format.\n",
        "- `DESCRIPTION (String / NaN)`\n",
        "A detailed textual description of the product, including specifications, features, and usage instructions.\n",
        "Example: \"Specifications: Color: Red, Material: Aluminium...\"\n",
        "- `PRODUCT_TYPE_ID (Integer / NaN)`\n",
        "A numeric identifier indicating the type or category of the product.\n",
        "Example: 1650, 2996, 7537\n",
        "- `PRODUCT_LENGTH (Float)`\n",
        "The length of the product, likely measured in millimeters or inches.\n",
        "Example: 2125.98, 393.7, 748.031495"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2OGXS6YSl5Y"
      },
      "source": [
        "### Steps:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Load and process the dataset using pandas\n",
        "\n",
        "2. **Vector Store Setup**:\n",
        "   - Convert product descriptions into embeddings.\n",
        "   - Store embeddings in a vector database.\n",
        "\n",
        "3. **Building the Chatbot**:\n",
        "   - Use LangChain to create an LLM pipeline.\n",
        "   - Develop a simple chatbot to answer product-related queries.\n",
        "\n",
        "4. **Creating a UI**:\n",
        "   - Implement a Gradio-based UI for user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ekric37zU6aV"
      },
      "outputs": [],
      "source": [
        "# Installing the LangChain Hub package to access and manage pre-built AI chains, prompts, and agents.\n",
        "!pip install langchainhub\n",
        "\n",
        "# Installing the LangChain OpenAI integration to use OpenAI models within LangChain workflows.\n",
        "!pip install langchain-openai\n",
        "\n",
        "# Installing the core LangChain library for building LLM-based applications, including chaining, memory, and retrieval capabilities.\n",
        "!pip install langchain\n",
        "\n",
        "# Installing the community version of LangChain, which includes integrations and tools contributed by the community.\n",
        "!pip install langchain-community\n",
        "\n",
        "# Installing FAISS (Facebook AI Similarity Search) for efficient similarity-based search on text embeddings.\n",
        "!pip install faiss-cpu\n",
        "\n",
        "# Installing Gradio, a framework to create web-based UIs for AI models and applications easily.\n",
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqC1rIL7IyXt"
      },
      "outputs": [],
      "source": [
        "# Importing the KaggleHub library to interact with datasets and models available on Kaggle.\n",
        "import kagglehub\n",
        "\n",
        "# Importing the CSV module for reading and writing CSV files.\n",
        "import csv\n",
        "\n",
        "# Importing pandas for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "\n",
        "# Importing numpy for numerical operations and handling arrays efficiently.\n",
        "import numpy as np\n",
        "\n",
        "# Importing os to interact with the operating system, such as environment variables and file paths.\n",
        "import os\n",
        "\n",
        "# Importing getpass to securely handle user input (e.g., API keys or passwords).\n",
        "import getpass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Vphy17StbK"
      },
      "source": [
        "### STEP 1: Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh9WpGhWRpCU"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XiPvpp1FxKb"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/datasets/sample_dataset.csv\",index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "MznuqvAmRx_g",
        "outputId": "b7c23f09-ed52-4521-fe2a-4f347ba2f3f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PRODUCT_ID                                              TITLE  \\\n",
              "0     1925202  ArtzFolio Tulip Flowers Blackout Curtain for D...   \n",
              "1     2673191  Marks & Spencer Girls' Pyjama Sets T86_2561C_N...   \n",
              "2     2765088  PRIKNIK Horn Red Electric Air Horn Compressor ...   \n",
              "3     1594019  ALISHAH Women's Cotton Ankle Length Leggings C...   \n",
              "4      283658  The United Empire Loyalists: A Chronicle of th...   \n",
              "\n",
              "                                       BULLET_POINTS  \\\n",
              "0  [LUXURIOUS & APPEALING: Beautiful custom-made ...   \n",
              "1  [Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...   \n",
              "2  [Loud Dual Tone Trumpet Horn, Compatible With ...   \n",
              "3  [Made By 95%cotton and 5% Lycra which gives yo...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                         DESCRIPTION  PRODUCT_TYPE_ID  \\\n",
              "0                                                NaN             1650   \n",
              "1                                                NaN             2755   \n",
              "2  Specifications: Color: Red, Material: Aluminiu...             7537   \n",
              "3  AISHAH Women's Lycra Cotton Ankel Leggings. Br...             2996   \n",
              "4                                                NaN             6112   \n",
              "\n",
              "   PRODUCT_LENGTH  \n",
              "0     2125.980000  \n",
              "1      393.700000  \n",
              "2      748.031495  \n",
              "3      787.401574  \n",
              "4      598.424000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9f6818a-7fef-4e8a-9c2c-7fca21ee91ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>BULLET_POINTS</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>PRODUCT_TYPE_ID</th>\n",
              "      <th>PRODUCT_LENGTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1925202</td>\n",
              "      <td>ArtzFolio Tulip Flowers Blackout Curtain for D...</td>\n",
              "      <td>[LUXURIOUS &amp; APPEALING: Beautiful custom-made ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1650</td>\n",
              "      <td>2125.980000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2673191</td>\n",
              "      <td>Marks &amp; Spencer Girls' Pyjama Sets T86_2561C_N...</td>\n",
              "      <td>[Harry Potter Hedwig Pyjamas (6-16 Yrs),100% c...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2755</td>\n",
              "      <td>393.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2765088</td>\n",
              "      <td>PRIKNIK Horn Red Electric Air Horn Compressor ...</td>\n",
              "      <td>[Loud Dual Tone Trumpet Horn, Compatible With ...</td>\n",
              "      <td>Specifications: Color: Red, Material: Aluminiu...</td>\n",
              "      <td>7537</td>\n",
              "      <td>748.031495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1594019</td>\n",
              "      <td>ALISHAH Women's Cotton Ankle Length Leggings C...</td>\n",
              "      <td>[Made By 95%cotton and 5% Lycra which gives yo...</td>\n",
              "      <td>AISHAH Women's Lycra Cotton Ankel Leggings. Br...</td>\n",
              "      <td>2996</td>\n",
              "      <td>787.401574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>283658</td>\n",
              "      <td>The United Empire Loyalists: A Chronicle of th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6112</td>\n",
              "      <td>598.424000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9f6818a-7fef-4e8a-9c2c-7fca21ee91ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9f6818a-7fef-4e8a-9c2c-7fca21ee91ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9f6818a-7fef-4e8a-9c2c-7fca21ee91ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f99fa3e0-fd0f-41fb-a7d1-fb0ba8e4aaad\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f99fa3e0-fd0f-41fb-a7d1-fb0ba8e4aaad')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f99fa3e0-fd0f-41fb-a7d1-fb0ba8e4aaad button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"PRODUCT_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 852274,\n        \"min\": 54125,\n        \"max\": 2998633,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          499846,\n          1033263,\n          2763742\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TITLE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Troop Leader Planner: A Complete Must-Have Troop Organizer, Dated Aug 2019 - Aug 2020\",\n          \"Fruit of the Loom Men&#39;s Fleece Full Zip Hoodie\",\n          \"Attiris Women's Satin Semi-Stitched Lehenga Choli (Green, Satin)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BULLET_POINTS\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 56,\n        \"samples\": [\n          \"[LUXURIOUS & APPEALING: Beautiful custom-made curtains to decorate any home or office | Includes inbuilt tieback to hold the curtain | Completely finished and ready to hang on walls & windows,MATERIAL: Luxurious & versatile fabric with a natural finish | High colour fastness | State-of-the-art digital printing ensures colour consistency and prevents any fading | Eyelets; Cotton Canvas; Width 4.5feet (54inch) | Multicolour | PACKAGE: 2 Room Curtains Eyelets | SIZE: Height 5 feet (60 inch); SET OF 2 PCS,BLACKOUT CURTAIN: 100% opaque & heavy premium cotton canvas fabric | Tight knitted, long life & durable fabric | Printing only on front side with a plain colour back side,MADE TO PERFECTION: Large eyelets at the top to put hanging hooks | Perfectly tailored seams for durability | Refined stitching with a matching thread color,QUALITY ASSURED: Gentle wash with similar colors in cold water | Avoid direct sunlight to prevent fading | Dispatched after MULTIPLE QUALITY CHECKS]\",\n          \"[HIGH QUALITY PVC MATERIAL: The kitchen aluminum foil stickers are made from plastic and aluminum foil, which is much stronger against high temperature than normal PVC sheet. The Anti-bacterial, Anti-mold, and Eco-friendly PVC materials is safe enough to use in Kitchen.,EASY TO USE AND CLEAN: Self-adhesive kitchen wallpaper is easy to stick on the dry, clean and smooth surface. Anti-oil, anti-dust and anti-water, anti-moisture PVC material allows you to wipe stains easily.,EASY CUT AND TRIM: back cut-to-fit grid lines design of the self-adhesive kitchen oil proof sticker is convenient for you to cut and trim any size you want. This silver rhombus texture add more fashion to your kitchen decor.,MULTI-USAGE: The kitchen backsplash wallpaper is great for home decorations to be used in kitchen cabinets ,countertop ,shelves, walls ,Pantry areas ,Dishwashers, Dishwasher Panels, Oven Hood, Refrigerators ,appliance etc, instant peel and apply to flat surfaces.,Anti-oil, anti-dust and anti-water: made from plastic and aluminum foil, much stronger against fire than normal PVC sheet. Resistant to heat and moisture, Easy wipe to remove the stains!]\",\n          \"[COMPLETELY BREATHABLE and 100% ECO-FRIENDLY-The breathable storage bags made with nonwon fabric that allows air to circulate inside, ultimately keeping stored items fresh for longer.Can folding in a small size, save more space if not in use.,DOUBLE STRONG ZIPPERS-The comforts storage bag has double smooth zippers so can open from the end or middle which make the storage bags easy to open and close.The zippers works well even with the bag full.Our zippers are much stronger and bigger than others.We can promise that ours can be used more than 1000 times (not like most cheap underbed bags where zipper break after only few uses).,EASY TO CARRY-Side double handles and front two handles make them easy to carry or grab and pull out from under the bed.,INSTANTLY KNOW WHAT'S INSIDE-Through the transparent top you can find the clothes, blankets, comforts conveniently and quickly without opening the blankets storage bags.,MULTIPURPOSE STORAGE OPTIONS-This underbed storage bag fits perfectly under your bed stacked on each other ,They are large enough that I fit 4 quilts (1 twin, 2 full/queen and a king) plus some pillowcases in just one of them..Also Works great for storing blankets and off season clothes.Dimensions: 39.37\\\"x19.68\\\"x5.90\\\" inches]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DESCRIPTION\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"<b>Under the Bed Storage Containers,Underbed Storage Bins Organizer for Blanket Clothes, Tidy Up Your Closet and Shelf,with Clear Window,2 Sturdy Zippers,4 Strong Handles Set of 2 Coffee with Lantern Printing</b><br> <b>COLOR:white with lantern printing</b><br> <b>SIZE:39.37\\\"x19.68\\\"x5.90\\\"</b><br> <b>Comforter breathable storage bag, Double strong zippers easy to carry or grab fits great under your bed</b><br> -Closet Soft Storage Bag With Clear Window Zippers and Handles ,classify Clothing,Comforters,Holiday Ornaments,Quilts, Blankets.<br> -Whether you want to tidy up your closets or create extra storage space under the bed, this versatile underbed organizer has got you covered in perfect style.<br> -Not only for under the bed, also for guestroom, down the attic stairs, your apartment with limited space to spare. They fold down to flat square when not in use.<br> -The jumbo blanket storage bag?is sturdy and durable.\",\n          \"Body by wacoal seamless underwire is sleek and modern\",\n          \"<p><b>Feature:</b></p><p>1. 480X320 HD resolution, only a few IO can light up the display<br>2. With memory card slot to facilitate the expansion of the experiment, provide a wealth of sample programs<br>3. Multifunctional use, can be used for other display functions through the signal transmission line<br>4. Small size, easy to carry and store, convenient to use and with good performance<br>5. Strict quality control and quality assurance, high safety factor, can be used with peace of mind</p><p><br></p><p><b>Spec:</b></p>Item Type: LCD Screen ModuleSize: Approx. 3.5 (inch)<br>Type: TFT<br>Driver Chip: ILI9488<br>Resolution: 480 x 320 (Pixel)<br>Module Interface: 4-wire SPI interface<br>Effective Display Area: (AA area) 48.96 x 73.44mm / 1.9 x 2.9in&nbsp;<br>Module PCB Floor Size: Approx.56.34 x 98mm / 2.2 x 3.9in&nbsp;<br>VCC Power Supply Voltage: 3.3V~5V<br>Logic I/O Voltage: 3.3V (TTL) <p><br></p><p><b>Package List:</b></p>1 x LCD Screen Module\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PRODUCT_TYPE_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3640,\n        \"min\": 0,\n        \"max\": 13101,\n        \"num_unique_values\": 87,\n        \"samples\": [\n          98,\n          1650,\n          2879\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PRODUCT_LENGTH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1370.245424206136,\n        \"min\": 10.0,\n        \"max\": 8858.25,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          640.0,\n          2125.98,\n          800.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Viewing the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3RpjmUS21M"
      },
      "source": [
        "**Constructing the text data**\n",
        "\n",
        "It's useful to use both `Title` and `Description`. To help downstream models understand which content is title and which content is description, we will add a prefix explaining which section is title and which is description. So each row should look like\n",
        "\n",
        "```\n",
        "Title\n",
        "{Title}\n",
        "Description\n",
        "{Description}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MduzbFnMSdNo"
      },
      "outputs": [],
      "source": [
        "## Let's construct the text data\n",
        "# Initializing empty lists to store product descriptions and their lengths\n",
        "product_description = []\n",
        "product_description_len = []\n",
        "\n",
        "# Iterating through each row in the dataframe df2\n",
        "for row in df2.iterrows():\n",
        "    product = \"\"  # Initialize an empty string to accumulate product details\n",
        "\n",
        "    # Extracting the product title from the current row\n",
        "    title = row[1][\"TITLE\"]\n",
        "\n",
        "    # Checking if the title is valid (not NaN or missing)\n",
        "    if type(title) != float or not math.isnan(title):\n",
        "        product += \"Title\\n\" + title + \"\\n\"  # Append the title to the product description\n",
        "\n",
        "    # Extracting the product description from the current row\n",
        "    description = row[1][\"DESCRIPTION\"]\n",
        "\n",
        "    # Checking if the description is valid (not NaN or missing)\n",
        "    if type(description) != float or not math.isnan(description):\n",
        "        product += \"Description\\n\" + description + \"\\n\"  # Append the description to the product details\n",
        "\n",
        "    # Check if either title or description was added\n",
        "    added_content = title or description\n",
        "    if added_content:\n",
        "        product = product.strip()  # Remove any leading/trailing whitespace\n",
        "        product_description.append(product)  # Add the formatted product details to the list\n",
        "        product_description_len.append(len(product))  # Store the length of the product description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd2Tpb2dTeQc",
        "outputId": "d248740f-f879-48c1-8955-3d14fa031e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of elements 100\n"
          ]
        }
      ],
      "source": [
        "# Checking the length of the data\n",
        "print(f\"Number of elements {len(product_description)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7JLY31mTx0l",
        "outputId": "faae5693-3944-4999-b1bf-75c46be5452b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title\n",
            "PRIKNIK Horn Red Electric Air Horn Compressor Interior Dual Tone Trumpet Loud Compatible with SX4\n",
            "Description\n",
            "Specifications: Color: Red, Material: Aluminium, Voltage: 12V, dB: 130 dB (around), Material: Aluminum Pump Head + Steel Pump Body + ABS Shell and Parts DB output: 130db Voltage: 12v Sound Type: Dual Tone Application: 12V Voltage Vehicles With Battery Above 20A Package included: 1 x Dual Tone Air Horn Compatible With SX4\n"
          ]
        }
      ],
      "source": [
        "# Check a sample product description data\n",
        "print(product_description[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaPPz4ebT0P4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6736fde-a183-449f-a776-67d34a8e2140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of items 100\n",
            "Min Length of the description: 18\n",
            "Avg Length of the description: 385.9\n",
            "Max Length of the description: 1834\n"
          ]
        }
      ],
      "source": [
        "# Print the total number of product descriptions processed\n",
        "print(\"Number of items\", len(product_description_len))\n",
        "\n",
        "# Print the minimum length of the product descriptions\n",
        "print(\"Min Length of the description:\",np.min(product_description_len))\n",
        "\n",
        "# Print the average (mean) length of the product descriptions\n",
        "print(\"Avg Length of the description:\",np.mean(product_description_len))\n",
        "\n",
        "# Print the maximum length of the product descriptions\n",
        "print(\"Max Length of the description:\",np.max(product_description_len))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4y4JSH_Svat"
      },
      "source": [
        "### Interpretation:\n",
        "\n",
        "What does the above result signify about the data?\n",
        "\n",
        "\n",
        "*   A minimum length of 18 suggests that some product descriptions might be too brief\n",
        "*  With an average length of 385.9 characters, most product descriptions contain a reasonable amount of information\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRjed6aXUo6j"
      },
      "source": [
        "### STEP 2: Vector Store Setup\n",
        "\n",
        "Let's try to get a few of the basic questions answered about vector stores before we start using it.\n",
        "\n",
        "### What is a vector store?\n",
        "A vector store is a specialized database that stores data in the form of numerical vectors, allowing efficient searching and retrieval based on similarity rather than exact matches.\n",
        "\n",
        "### Why do we need a vector store?\n",
        "Traditional databases rely on exact keyword matches, which can miss relevant information. A vector store helps find similar content by understanding relationships and meaning in data.\n",
        "\n",
        "### How does a vector store work?\n",
        "It converts text, images, or other data into numerical vectors using AI models, then stores these vectors and retrieves similar ones using techniques like cosine similarity.\n",
        "\n",
        "### How does a vector store improve search results?\n",
        "It enables searches based on meaning rather than just keywords, providing more relevant results even if the exact terms don't match.\n",
        "\n",
        "### What are some popular vector store tools?\n",
        "- FAISS (Facebook AI Similarity Search)\n",
        "- Pinecone\n",
        "- Weaviate\n",
        "- Chroma\n",
        "\n",
        "### What is an embedding, and how does it relate to a vector store?\n",
        "An embedding is a numerical representation of data (e.g., text, image) that captures its meaning. These embeddings are stored in a vector store for efficient retrieval.\n",
        "\n",
        "\n",
        "Our next step is\n",
        "-  to convert the `product_description` to chunks\n",
        "-  convert each chunk to embedding\n",
        "-  store it in vector store for searching\n",
        "\n",
        "As discussed earlier we shall use `LangChain` to perform these steps.\n",
        "\n",
        "LangChain is a framework that helps developers build applications powered by large language models (LLMs) like GPT by providing tools for various tasks to be carried out like retrieving relevant information from databases, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwFdqweOT2xI"
      },
      "outputs": [],
      "source": [
        "# Importing RecursiveCharacterTextSplitter from LangChain for chunking large text into smaller, manageable pieces.\n",
        "# This helps in optimizing text for processing and retrieval.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Importing OpenAIEmbeddings from LangChain to generate numerical vector representations (embeddings) of text.\n",
        "# These embeddings capture the semantic meaning of the text for efficient similarity searches.\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Importing FAISS (Facebook AI Similarity Search) from LangChain's community package.\n",
        "# FAISS is used for storing and retrieving embeddings efficiently by finding similar vectors.\n",
        "from langchain_community.vectorstores import FAISS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X3hxmTTUsr1"
      },
      "outputs": [],
      "source": [
        "# Setting the OpenAI API key as an environment variable.\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD-uoBzLVtdw"
      },
      "outputs": [],
      "source": [
        "# Split the input text using Recursive Character Chunking\n",
        "# See this for more details https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=250,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "documents = text_splitter.create_documents(product_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Explanation:\n",
        "The above code initializes a `RecursiveCharacterTextSplitter` to break down product_description into smaller text chunks of 250 characters each, with a 20-character overlap to preserve context between chunks. The `create_documents` function processes the text list and generates structured document chunks for efficient retrieval and analysis.\n",
        "\n",
        "### Why do we need overlap?\n",
        "Overlap is needed to ensure continuity and preserve context between chunks, preventing important information from being cut off at chunk boundaries. This helps AI models better understand the text when processing each chunk independently, improving retrieval accuracy and response quality."
      ],
      "metadata": {
        "id": "MqtfA0tXh7CL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptE8XpNLWD--"
      },
      "outputs": [],
      "source": [
        "# Create an embedding model using LangChain.\n",
        "# One option is using https://python.langchain.com/docs/integrations/text_embedding/openai/\n",
        "# See https://python.langchain.com/docs/integrations/text_embedding/ for a list of available embedding models on LangChain\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSlO9vblWGDt"
      },
      "outputs": [],
      "source": [
        "# Create a vector store using the created chunks and the embeddings model\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What have we done so far?\n",
        "1. Data Preparation: Extracted the product description data\n",
        "2. Data Chunking: Converted the entire data into multiple manageable chunks\n",
        "3. Chunks to Embeddings: Converted the broken down chunks into embeddings\n",
        "4. Storage in a Vector DB: Stored the resulting embeddings of chunks in a vector store for effective retrieval.\n",
        "\n",
        "\n",
        "### What is remaining?\n",
        "- Building the chatbot\n",
        "- Building the Gradio UI"
      ],
      "metadata": {
        "id": "nNhCKF9Nig_n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c24S9JsfWSa7"
      },
      "source": [
        "### STEP 3: Building the chatbot\n",
        "\n",
        "Now that we have converted the documents to embeddings, our next step is to\n",
        "- build a retriever that uses the vector store to retrieve the documents\n",
        "- create a prompt template that contains the augmented context using the retrieved documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nElsMdbeW4l1"
      },
      "outputs": [],
      "source": [
        "# Importing ChatOpenAI from LangChain to interact with OpenAI's language models, such as GPT, for generating responses.\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Importing ChatPromptTemplate to create structured prompts for the chatbot, ensuring consistent interactions with the AI model.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Importing OpenAIEmbeddings to convert text data into numerical vector representations for similarity search and retrieval.\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Importing ChatPromptTemplate again (duplicate import, should be removed to avoid redundancy).\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Importing create_stuff_documents_chain to combine and process retrieved documents for meaningful AI-generated responses.\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Importing create_retrieval_chain to build a chain that retrieves relevant documents from a vector store and generates AI responses.\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "# Importing StrOutputParser from LangChain to parse the output\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code Explanation:\n",
        "- `ChatOpenAI` – Used to access OpenAI models for chatbot functionality.\n",
        "- `ChatPromptTemplate` – Helps structure queries to ensure better responses.\n",
        "- `OpenAIEmbeddings` – Converts text into vector form for similarity-based retrieval.\n",
        "- `create_stuff_documents_chain` – Combines retrieved documents meaningfully before passing to the LLM.\n",
        "- `create_retrieval_chain` – Automates the process of retrieving and utilizing relevant content for AI responses.\n",
        "- `StrOutputParser` - For processing the output of language models, ensuring that the output is returned as a plain string"
      ],
      "metadata": {
        "id": "xVChvJAijx-1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dJXzILCXStu"
      },
      "outputs": [],
      "source": [
        "# Initializing the ChatOpenAI model to interact with OpenAI's GPT model.\n",
        "llm = ChatOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], model = 'gpt-4o-mini')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the output parser to process and format the model's response into a readable string format.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Creating a prompt template that instructs the AI to act as a customer service agent.\n",
        "# The prompt takes two parameters:\n",
        "#   1. {context} - Relevant information retrieved from the document store.\n",
        "#   2. {input} - The user's question.\n",
        "# The model is instructed to base its answer solely on the provided context.\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "    <context>\n",
        "    {context}\n",
        "    </context>\n",
        "\n",
        "    Question: {input}\"\"\",\n",
        "    output_parser=output_parser  # The output parser ensures that the response is returned in a structured string format.\n",
        ")\n",
        "\n",
        "# Creating a document processing chain using the LLM and the defined prompt template.\n",
        "# This chain takes a list of retrieved documents and passes them as context to the model for generating responses.\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# Alternative chain creation method:\n",
        "# Using the \"|\" (pipe) operator to link the prompt with the language model (llm),\n",
        "# meaning the input first goes to the prompt and then to the model for response generation.\n",
        "# document_chain = prompt | llm\n"
      ],
      "metadata": {
        "id": "dPJ3yFeEk9EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code Explanation:\n",
        "- A structured prompt is created using `ChatPromptTemplate` to guide the AI in answering questions based solely on provided context.\n",
        "- The prompt includes placeholders `{context}` and `{input}` to dynamically inject relevant information.\n",
        "- `StrOutputParser()` ensures that the AI's response is formatted as plain text for easy processing and display.\n",
        "- `create_stuff_documents_chain(llm, prompt)` combines the language model (LLM) with the prompt to form a processing chain. This chain takes retrieved documents as input and generates AI-driven responses.\n",
        "- Alternate way:  `prompt | llm` is a more concise way to chain the prompt and model, achieving the same functionality with a cleaner syntax."
      ],
      "metadata": {
        "id": "__HKSf_Umiq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever from the vector store for fetching relevant documents\n",
        "# See https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/\n",
        "retriever = vector.as_retriever()\n",
        "\n",
        "# Create a retrieval chain that first retrieves relevant documents and then processes them using the document chain\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
      ],
      "metadata": {
        "id": "VMiWghjrl5oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code Explanation:\n",
        "- The `vector.as_retriever()` converts the vector store into a retriever to find documents based on query similarity.\n",
        "- The `create_retrieval_chain()` connects the retriever with the document processing pipeline, ensuring the AI receives relevant context before generating responses.\n",
        "\n",
        "This setup enables the AI to provide accurate answers by first retrieving and then processing relevant documents."
      ],
      "metadata": {
        "id": "xY3X-YJZmT3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUqxhEW3XcJX",
        "outputId": "fe1c6890-7572-474e-f04d-ac283e372e52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what are some of the best shoes available?',\n",
              " 'context': [Document(id='836197e0-2c81-4836-998f-fe715cbca2a8', metadata={}, page_content=\"Title\\nadidas Men's Predator 18+ FG Firm Ground Soccer Cleats\\nDescription\\nadidas Predator 18+ FG- Black 7.5\"),\n",
              "  Document(id='d1ab1a65-1519-488d-abd4-15b2c161f9c4', metadata={}, page_content=\"Title\\nPUMA Cali Sport Clean Women's Sneakers White Leather (37540701)\"),\n",
              "  Document(id='c55274dc-a120-4a67-9c4c-da52c03f9e15', metadata={}, page_content=\"Title\\nKenneth Cole REACTION Men's Crespo Loafer B Shoe, Cognac, 10 M US\"),\n",
              "  Document(id='40703760-e719-4b6b-ac42-3f38f8d0922e', metadata={}, page_content=\"The Remora Climbing Shoe is Mad Rock's do-it-all slipper for climbers who can't have separate shoes for boulders, sport routes, and gyms. With a moderately stiff, slightly downturned design, the Remora performs on any climb at steep to vertical\")],\n",
              " 'answer': \"Based on the provided context, some of the best shoes available include:\\n\\n1. adidas Men's Predator 18+ FG Firm Ground Soccer Cleats\\n2. PUMA Cali Sport Clean Women's Sneakers\\n3. Kenneth Cole REACTION Men's Crespo Loafer B Shoe\\n4. Mad Rock Remora Climbing Shoe\\n\\nThese options represent a variety of shoe types suitable for different activities.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# Invoking the retrieval chain to process the user's query.\n",
        "# The query \"what are some of the best shoes available?\" is passed as input.\n",
        "# The retrieval chain first fetches relevant product descriptions from the vector store,\n",
        "# then processes them using the document chain to generate a meaningful AI response.\n",
        "retrieval_chain.invoke({\"input\": \"what are some of the best shoes available?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "QqNvod6GXfjx",
        "outputId": "083232f4-32d8-4f5c-cfd6-e72f372f27ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the provided context, some of the best shoes available include:\\n\\n1. adidas Men's Predator 18+ FG Firm Ground Soccer Cleats\\n2. PUMA Cali Sport Clean Women's Sneakers\\n3. Kenneth Cole REACTION Men's Crespo Loafer B Shoe\\n4. Mad Rock Remora Climbing Shoe\\n\\nEach of these options serves different athletic and casual purposes.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# Fetching the final answer from the retrieval chain by invoking it with a user query.\n",
        "# The ['answer'] key extracts the final AI-generated answer from the response dictionary.\n",
        "retrieval_chain.invoke({\"input\": \"what are some of the best shoes available?\"})['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we got the answer! But, the formatting is not very good, right? Lets create a simple UI for our bot."
      ],
      "metadata": {
        "id": "2tMQ3SGUnoad"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvDsSk-bYSwS"
      },
      "source": [
        "### STEP 4: Building a simple Gradio UI\n",
        "\n",
        "Gradio is an open-source Python library that makes it easy to build interactive user interfaces for machine learning models, APIs, and data science workflows. It allows developers to create shareable web-based UIs with just a few lines of code.\n",
        "\n",
        "To build the gradio app we'll utilize the following steps:\n",
        "\n",
        "- Modularize the entire RAG pipeline using a single function\n",
        "- Create the building blocks for the UI.\n",
        "- Connect the UI with the function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the user query and return formatted product names\n",
        "def final_response(user_query):\n",
        "    # Invoking the retrieval chain with the user's query to fetch relevant product information\n",
        "    response = retrieval_chain.invoke({\"input\": user_query})['answer']\n",
        "\n",
        "    # Creating a prompt to instruct the AI to format the response properly\n",
        "    # The prompt asks the AI to extract only product names from the retrieved response\n",
        "    prompt = f\"Format the responses properly in {response}. Just return the product names, no other text\"\n",
        "\n",
        "    # Sending the formatted prompt to the GPT-4o-mini model for processing\n",
        "    openai_response = client.chat.completions.create(\n",
        "        model='gpt-4o-mini',  # Using GPT-4o-mini model for response generation\n",
        "        messages=[{'role': 'user', 'content': prompt}]  # Providing the prompt to the model\n",
        "    )\n",
        "\n",
        "    # Extracting and returning the AI-generated response containing only the product names\n",
        "    return openai_response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "PcAbbKmKoHZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttosPCA2bLYz",
        "outputId": "56abf270-05ef-4e1c-a0ad-8b3277d0d59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. adidas Men's Predator 18+ FG Firm Ground Soccer Cleats  \n",
            "2. PUMA Cali Sport Clean Women's Sneakers  \n",
            "3. Kenneth Cole REACTION Men's Crespo Loafer B Shoe  \n",
            "4. Mad Rock Remora Climbing Shoe  \n"
          ]
        }
      ],
      "source": [
        "# Printing the final response\n",
        "print(final_response(\"what are some of the best shoes available?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "jjS_RUh6ZstZ",
        "outputId": "0e55f467-1348-4f9f-b32e-623daec43ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8f67ef0d08fb3930ed.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8f67ef0d08fb3930ed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# Importing the Gradio library to create a simple web-based user interface\n",
        "import gradio as gr\n",
        "\n",
        "# Creating the Gradio interface for the product recommendation system\n",
        "app = gr.Interface(\n",
        "    fn=final_response,        # The function that processes user input and returns recommendations\n",
        "    inputs=\"text\",            # Input component: a text box for users to enter their query\n",
        "    outputs=\"text\",           # Output component: a text box to display the AI-generated response\n",
        "    title=\"Review Genie\",     # The title of the web interface\n",
        "    description=\"Type your question below to get the recommendations\",# A brief description displayed to users\n",
        "    theme=\"Ocean\",\n",
        "    allow_flagging=\"never\"    # Disabling the flagging feature to remove the \"Flag\" button\n",
        ")\n",
        "\n",
        "# Launching the Gradio app to start the interface and make it accessible via web browser\n",
        "app.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}